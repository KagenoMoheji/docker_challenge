version: "3"

services:
  spark-driver:
    build:
      context: .
      dockerfile:
        # docker-compose.ymlからの相対パスにする
        ./docker/spark/driver/Dockerfile
      args:
        JAVA_VERSION: ${JAVA_VERSION}
        SPARK_VERSION: ${SPARK_VERSION}
        HADOOP_VERSION: ${HADOOP_VERSION}
        PYTHON_VERSION: ${PYTHON_VERSION}
    image: spark-driver
    volumes:
      # ノード間の共有ディレクトリ？
      - ./docker/spark/shared-workspace:/opt/workspace
      # Driverノードにだけ開発ディレクトリマウントしておく
      # - ./src:/src
    ports:
      - 4040:4040
      - 8080:8080
    command:
      "/spark/bin/spark-class org.apache.spark.deploy.master.Master -h 0.0.0.0"

  spark-worker:
    build:
      context: .
      dockerfile: ./docker/spark/worker/Dockerfile
      args:
        JAVA_VERSION: ${JAVA_VERSION}
        SPARK_VERSION: ${SPARK_VERSION}
        HADOOP_VERSION: ${HADOOP_VERSION}
    image: spark-worker
    volumes:
      # ノード間の共有ディレクトリ？
      - ./docker/spark/shared-workspace:/opt/workspace
    depends_on:
      - spark-driver
    ports:
      # 下記の書き方で，左のポート範囲のうち使用可能なポートを選択してくれる．
      # つまり「8081-8089」なら9つまでのワーカーノードを作成できることを意味する．
      - 8081-8089:8081
    command:
      # 「spark://XXX」のXXXにはdriver(master)ノードのサービス名を指定．
      "/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-driver:7077 -h 0.0.0.0 -c ${SPARK_WORKER_CORES} -m ${SPARK_WORKER_MEMORY}"


